# Инструкция по установке и настройке Ollama

Для работы функции генерации протоколов встреч необходимо установить и настроить Ollama - инструмент для локального запуска языковых моделей.

## 1. Установка Ollama

### Windows
1. Скачайте установщик с официального сайта: https://ollama.com/download/windows
2. Запустите скачанный файл и следуйте инструкциям установщика
3. После установки Ollama будет запущен автоматически (значок появится в системном трее)

### macOS
```bash
curl -fsSL https://ollama.com/install.sh | sh
```

### Linux
```bash
curl -fsSL https://ollama.com/install.sh | sh
```

## 2. Загрузка модели

После установки Ollama необходимо загрузить языковую модель. Рекомендуется использовать llama3 или mistral:

```bash
# Для загрузки Llama 3 (8B)
ollama pull llama3

# ИЛИ для загрузки Mistral
ollama pull mistral
```

Загрузка модели может занять некоторое время в зависимости от скорости вашего интернет-соединения.

## 3. Проверка работоспособности

Чтобы убедиться, что Ollama работает корректно, выполните следующую команду:

```bash
ollama run llama3 "Привет, как дела?"
```

Вы должны получить осмысленный ответ от модели.

## 4. Настройка для работы с ботом

1. Убедитесь, что Ollama запущен и работает в фоновом режиме
2. По умолчанию Ollama API доступен по адресу http://localhost:11434
3. Если вы изменили порт или хост, обновите соответствующие настройки в файле config.py

## 5. Дополнительные зависимости

Для работы с PDF необходимо установить дополнительные зависимости:

```bash
pip install weasyprint markdown
```

## Примечания

- Ollama должен быть запущен до запуска бота
- Первый запрос к модели может занять больше времени, последующие будут быстрее
- Для лучшего качества генерации протоколов рекомендуется использовать модель llama3
- Требования к системе: минимум 8 ГБ оперативной памяти, рекомендуется 16 ГБ
